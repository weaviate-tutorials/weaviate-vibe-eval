from typing import Dict, Any, List, Optional
import re

from weaviate_vibe_eval.models.model import BaseModel, ModelNames, AnthropicModel

class JudgeModel:
    """
    LLM-based judge for evaluating generated Weaviate code against canonical implementations.
    """

    def __init__(
        self,
        model: Optional[BaseModel] = None,
        model_name: ModelNames = ModelNames.CLAUDE_3_7_SONNET_20250219,
        evaluation_criteria: Optional[List[str]] = None,
    ):
        """
        Initialize the judge model.

        Args:
            model: An existing model instance to use as judge (optional)
            model_name: Model to use if no existing instance is provided
            evaluation_criteria: List of criteria to evaluate (if None, uses defaults)
        """
        self.model = model or AnthropicModel(model_name=model_name)
        self.evaluation_criteria = evaluation_criteria or [
            "Correctness",
            "Style & Best Practices",
            "Error Handling",
            "Efficiency",
            "API Usage"
        ]

    def evaluate_code(
        self,
        generated_code: str,
        canonical_code: str,
        task_description: str,
        execution_result: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Evaluate generated code against canonical implementation.

        Args:
            generated_code: Code generated by the model being evaluated
            canonical_code: Reference/canonical implementation
            task_description: Description of the task
            execution_result: Output from executing the generated code (optional)

        Returns:
            Dictionary with evaluation results
        """
        prompt = self._create_evaluation_prompt(
            generated_code,
            canonical_code,
            task_description,
            execution_result
        )

        evaluation_text = self.model.generate(prompt, temperature=0.1)

        # Parse the evaluation text to extract structured results
        parsed_results = self._parse_evaluation(evaluation_text)

        return {
            "raw_evaluation": evaluation_text,
            "scores": parsed_results.get("scores", {}),
            "overall_score": parsed_results.get("overall_score", 0),
            "reasoning": parsed_results.get("reasoning", ""),
            "suggestions": parsed_results.get("suggestions", [])
        }

    def _create_evaluation_prompt(
        self,
        generated_code: str,
        canonical_code: str,
        task_description: str,
        execution_result: Optional[Dict[str, Any]] = None
    ) -> str:
        """Create a prompt for the judge model to evaluate code."""
        execution_info = ""
        if execution_result:
            stdout = execution_result.get("stdout", "")
            stderr = execution_result.get("stderr", "")
            exit_code = execution_result.get("exit_code", "")
            execution_info = f"""
### Execution Results
Exit Code: {exit_code}
Standard Output:
```
{stdout}
```
Standard Error:
```
{stderr}
```
"""

        criteria_text = "\n".join([f"- {criterion}" for criterion in self.evaluation_criteria])

        return f"""You are an expert code evaluator. Your task is to evaluate generated Python code for a Weaviate task against a canonical implementation.

### Task Description
{task_description}

### Generated Code
```python
{generated_code}
```

### Canonical Implementation
```python
{canonical_code}
```
{execution_info}

### Evaluation Criteria
{criteria_text}

### Instructions
1. Evaluate the generated code against the canonical implementation.
2. For each criterion, assign a score from 1-5 (where 5 is best).
3. Provide an overall score from 1-5.
4. Explain your reasoning.
5. Provide specific suggestions for improvement.

Please format your response as follows:
```
Scores:
- Criterion1: X/5
- Criterion2: X/5
...
Overall: X/5

Reasoning:
[Your detailed analysis]

Suggestions:
- [Suggestion 1]
- [Suggestion 2]
...
```
"""

    def _parse_evaluation(self, evaluation_text: str) -> Dict[str, Any]:
        """Parse the evaluation text into a structured format."""
        result = {
            "scores": {},
            "overall_score": 0,
            "reasoning": "",
            "suggestions": []
        }

        # Extract scores section
        scores_match = evaluation_text.split("Reasoning:", 1)[0] if "Reasoning:" in evaluation_text else evaluation_text

        # Parse individual criterion scores
        for criterion in self.evaluation_criteria:
            pattern = fr"{criterion}:\s*(\d+)/5"
            match = re.search(pattern, scores_match, re.IGNORECASE)
            if match:
                result["scores"][criterion] = int(match.group(1))

        # Extract overall score
        overall_match = re.search(r"Overall:\s*(\d+)/5", scores_match, re.IGNORECASE)
        if overall_match:
            result["overall_score"] = int(overall_match.group(1))

        # Extract reasoning
        if "Reasoning:" in evaluation_text and "Suggestions:" in evaluation_text:
            result["reasoning"] = evaluation_text.split("Reasoning:", 1)[1].split("Suggestions:", 1)[0].strip()

        # Extract suggestions
        if "Suggestions:" in evaluation_text:
            suggestions_text = evaluation_text.split("Suggestions:", 1)[1].strip()
            # Extract bullet points
            suggestions = re.findall(r"- (.*?)(?=$|\n- )", suggestions_text, re.DOTALL)
            result["suggestions"] = [s.strip() for s in suggestions]

        return result
